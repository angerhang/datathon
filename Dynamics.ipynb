{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "print(\"PyTorch version: %s\" % torch.__version__)\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device: %s\" % dev)\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "\n",
    "### classes ###\n",
    "\n",
    "# watch out for neuron firing before spike arriving at the target of its synapses\n",
    "class Recurrent(nn.Module):\n",
    " \n",
    "    def __init__(self, neuron_groups, delta_t, noise_tau, trail_num=1, w_ij=None, axon_delays=None):\n",
    "        super(Recurrent, self).__init__()\n",
    "        \n",
    "        self.num_neurons = neuron_groups[:, 0].type(torch.IntTensor)\n",
    "        self.tot_neurons = torch.sum(self.num_neurons).item()\n",
    "        self.trial_num = trail_num\n",
    "        \n",
    "        self.d_t = torch.tensor(delta_t, device=dev)\n",
    "        self.n_sqdt_tau = torch.tensor(math.sqrt(delta_t) / noise_tau, device=dev)\n",
    "        self.n_decay = torch.tensor(math.exp(-delta_t / noise_tau), device=dev)\n",
    "        \n",
    "        self.n_m = Parameter(self.d_t * torch.eye(self.tot_neurons, device=dev))\n",
    "        self.subs = torch.ones((self.trial_num, self.tot_neurons, self.tot_neurons), device=dev)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.threshold = torch.empty((self.trial_num, self.tot_neurons), device=dev)\n",
    "        self.dt_tau = torch.empty((self.trial_num, self.tot_neurons), device=dev)\n",
    "        self.decay = torch.empty((self.trial_num, self.tot_neurons), device=dev)\n",
    "        self.refractory = torch.empty((self.trial_num, self.tot_neurons), device=dev)\n",
    "        p_sz = 0\n",
    "        cnt = 0\n",
    "        for sz in self.num_neurons:\n",
    "            self.threshold[:, p_sz : p_sz + sz] = neuron_groups[cnt, 1]\n",
    "            self.dt_tau[:, p_sz : p_sz + sz] = delta_t / neuron_groups[cnt, 2]\n",
    "            self.decay[:, p_sz : p_sz + sz] = math.exp(-delta_t / neuron_groups[cnt, 2])\n",
    "            self.refractory[:, p_sz : p_sz + sz] = neuron_groups[cnt, 3]\n",
    "            p_sz += sz\n",
    "            cnt += 1\n",
    "        self.num_groups = cnt\n",
    "\n",
    "        self.grad_clip = torch.ones((self.tot_neurons, self.tot_neurons), device=dev)\n",
    "        if w_ij is not None:\n",
    "            self.w = Parameter(torch.empty((self.tot_neurons, self.tot_neurons), device=dev))\n",
    "            self.axon_delays = torch.empty((self.tot_neurons, self.tot_neurons), device=dev)\n",
    "            for i in range(self.tot_neurons):\n",
    "                for j in range(self.tot_neurons):\n",
    "                    if w_ij[i,j] == 0:\n",
    "                        self.grad_clip[i,j] = 0.0\n",
    "                    self.w.data[i,j] = w_ij[i,j]\n",
    "                    self.axon_delays.data[i,j] = c_ij[i,j]\n",
    "        else:\n",
    "            self.w = Parameter(torch.randn((self.tot_neurons, self.tot_neurons), device=dev))\n",
    "            self.axon_delays = torch.randint(1, 100, (self.tot_neurons, self.tot_neurons), device=dev)\n",
    "        \n",
    "        # generate the boolean indicator for E/I cells\n",
    "        self.dale = torch.zeros(self.tot_neurons).type(torch.ByteTensor)\n",
    "        for j in range(self.tot_neurons):\n",
    "            if torch.sum(self.w[:,j]) > 0:\n",
    "                 self.dale[j] = True\n",
    "                \n",
    "    def reset(self, init_fluc):\n",
    "        self.h = (2.0 * torch.rand((self.trial_num, self.tot_neurons), device=dev) - 1.0) * init_fluc\n",
    "        self.eta = torch.zeros((self.trial_num, self.tot_neurons), device=dev)\n",
    "        self.refract = torch.zeros((self.trial_num, self.tot_neurons), device=dev)\n",
    "        self.axons = torch.zeros((self.trial_num, self.tot_neurons, self.tot_neurons), device=dev)\n",
    "        self.fires = (self.h >= self.threshold)\n",
    " \n",
    "    def constraints(self):\n",
    "        self.w.data = self.w.data * self.grad_clip\n",
    "        self.n_m.data = torch.triu(self.n_m.data)\n",
    "        \n",
    "        # enforce Dale's law by clamping\n",
    "        for j in range(self.tot_neurons):\n",
    "            if self.dale[j]: # E\n",
    "                self.w.data[self.w.data[:,j] < 0, j] = 0\n",
    "            else: # I\n",
    "                self.w.data[self.w.data[:,j] > 0, j] = 0\n",
    "            # must set .data to value, otherwise we get rid of leaf node! Autograd complains\n",
    "        \n",
    "    def forward(self, u_ext):\n",
    "        noise = torch.randn((self.trial_num, self.tot_neurons), device=dev)\n",
    "        self.eta = self.n_decay * self.eta + self.n_sqdt_tau * torch.matmul(noise, self.n_m.t())\n",
    "        \n",
    "        self.h[self.fires] = 0.0\n",
    "        for t in range(self.trial_num):\n",
    "            for j in range(self.tot_neurons):\n",
    "                if self.fires[t,j]: # only one AP down axons needs high enough refractory\n",
    "                    self.axons[t, j] = self.axon_delays[:, j] # axons tji, delays ij labeling\n",
    "        \n",
    "        spikes = (self.axons == 1).sum(1).type(torch.cuda.FloatTensor) # presynaptic spikes\n",
    "        self.axons = self.relu(self.axons - self.subs)\n",
    "        \n",
    "        refr = (self.refract > 0)\n",
    "        refr_f = ~refr\n",
    "        dh = (self.decay * self.h + self.dt_tau * (torch.matmul(spikes, self.w.t()) + u_ext + self.eta))\n",
    "        self.h[refr_f] = dh[refr_f]\n",
    "        \n",
    "        self.fires = (self.h >= self.threshold) # soma fires\n",
    "        self.refract[self.fires] = self.refractory[self.fires]\n",
    "        self.refract = self.refract - refr.to(dev).type(torch.cuda.FloatTensor)\n",
    "\n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, neuron_groups, neurons_out, trail_num, w_ij, c_ij):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.num_neurons = neuron_groups[:, 0].type(torch.int8)\n",
    "        self.neurons_in = torch.sum(self.num_neurons).item()\n",
    "        self.trial_num = trail_num\n",
    "        \n",
    "        self.alpha = torch.empty((trial_num, self.neurons_in), device=dev)\n",
    "        p_sz = 0\n",
    "        cnt = 0\n",
    "        for sz in self.num_neurons:\n",
    "            self.alpha[:, p_sz : p_sz + sz] = neuron_groups[cnt, 1]\n",
    "            p_sz += sz\n",
    "            cnt += 1\n",
    "        \n",
    "        self.w = Parameter(torch.empty((neurons_out, neurons_in), device=dev))\n",
    "        self.axon_delays = torch.empty((self.tot_neurons, self.tot_neurons), device=dev)\n",
    "        \n",
    "        self.subs = torch.ones((self.trial_num, self.tot_neurons, self.tot_neurons), device=dev)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.grad_clip = torch.zeros((neurons_out, neurons_in), device=dev)\n",
    "        \n",
    "        for i in range(neurons_out):\n",
    "            for j in range(self.neurons_in):\n",
    "                if w_ij[i,j] == 0:\n",
    "                    self.grad_clip[i,j] = 1.0\n",
    "                self.w.data[i,j] = w_ij[i,j]\n",
    "                self.axon_delays.data[i,j] = c_ij[i,j]                 \n",
    "        \n",
    "        # generate the boolean indicator for E/I cells\n",
    "        self.dale = torch.zeros(neurons_in).type(torch.ByteTensor)\n",
    "        for j in range(self.neurons_in):\n",
    "            if torch.sum(self.w[:,j]) > 0:\n",
    "                self.dale[j] = True\n",
    "    \n",
    "    def reset(self, init_fluc):\n",
    "        self.axons = torch.zeros((self.trial_num, self.tot_neurons, self.tot_neurons), device=dev)\n",
    "        \n",
    "    def constraints(self):\n",
    "        self.w.data = self.w.data * self.grad_clip\n",
    "        # enforce Dale's law by clamping\n",
    "        for j in range(self.neurons_in):\n",
    "            if self.dale[j]: # E\n",
    "                self.w.data[self.w.data[:,j] < 0, j] = 0\n",
    "            else: # I\n",
    "                self.w.data[self.w.data[:,j] > 0, j] = 0\n",
    "        \n",
    "    def forward(self, fires):\n",
    "        for t in range(self.trial_num):\n",
    "            for j in range(self.tot_neurons):\n",
    "                if fires[t,j]:\n",
    "                    self.axons[t, j] = self.axon_delays[:, j] # axons tji, delays ij labeling\n",
    "        \n",
    "        spikes = (self.axons == 1).sum(1).type(torch.cuda.FloatTensor) # presynaptic spikes\n",
    "        self.axons = self.relu(self.axons - self.subs)\n",
    "        \n",
    "        return torch.matmul(spikes, self.w.t())\n",
    "        \n",
    "\n",
    "### initialization ###\n",
    "T = 5000\n",
    "del_T = 1\n",
    "offset = 0\n",
    "latent_size = 50\n",
    "\n",
    "d_t = 0.2\n",
    "neuron_data = torch.tensor([[latent_size, 0.01, 20.0, 100], [latent_size, 0.2, 10.0, 200]])\n",
    "neurons = int(torch.sum(neuron_data[:, 0]).item())\n",
    "\n",
    "w_ij = torch.zeros((neurons, neurons))\n",
    "w_ij[:, 0:latent_size] = 10.0\n",
    "w_ij[:, latent_size:neurons] = -10.0\n",
    "for k in range(0, neurons):\n",
    "    w_ij[k, k] = 0.0\n",
    "    \n",
    "c_ij = torch.zeros((neurons, neurons))\n",
    "c_ij[:, 0:latent_size] = 3\n",
    "c_ij[:, latent_size:neurons] = 1\n",
    "for k in range(0, neurons):\n",
    "    c_ij[k, k] = 0\n",
    "\n",
    "trials = 1\n",
    "\n",
    "model = Recurrent(neuron_data, d_t, 20.0, trials)#, w_ij, c_ij)\n",
    "        \n",
    "\n",
    "#I = torch.tensor([1.0,1.0,1.0, 0.0,0.0,0.0], device=dev)\n",
    "### simulation ###\n",
    "h_sav = []\n",
    "f_sav = []\n",
    "model.reset(0.0)\n",
    "model.constraints()\n",
    "\n",
    "I = torch.tensor([10.0,10.0,10.0,0.0,0.0,0.0], device=dev)\n",
    "for t in range(T + 1):\n",
    "    if t % del_T == 0:\n",
    "        h_sav.append(model.h.cpu().data.numpy())\n",
    "        f_sav.append(model.fires.cpu().data.numpy())\n",
    "    if t == T:\n",
    "        break\n",
    "    if t < 0:\n",
    "        model(0.0)\n",
    "    else:\n",
    "        model(0)\n",
    "\n",
    "#h_sav = torch.stack(h_sav)\n",
    "h = np.asarray(h_sav)\n",
    "f = np.asarray(f_sav)\n",
    "\n",
    "\n",
    "\n",
    "### visualization ###\n",
    "v = range(h.shape[0])\n",
    "plt.plot(v, h[:, 0, 0])\n",
    "\n",
    "width = h.shape[0] / h.shape[2]\n",
    "height =  5\n",
    "plt.figure(figsize=(width, height))\n",
    "a = f[:,0,:]\n",
    "plt.imshow(a.transpose(), interpolation=\"nearest\", cmap=plt.cm.gray)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Neuron\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
